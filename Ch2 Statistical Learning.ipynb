{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6918cb-59d0-4433-8214-eb480ed4663c",
   "metadata": {},
   "source": [
    "# Chapter 2: Statistical Learning\n",
    "\n",
    "## General form\n",
    "$$\n",
    "Y = f(X) + \\epsilon \n",
    "$$\n",
    "We estimate f for: (1) prediction (2) inference\n",
    "\n",
    "## Parametric Methods and Non-premetric Methods\n",
    "Paremetric methods\n",
    "- make an assumption about the functional form\n",
    "- fit/train data to get parameters\n",
    "\n",
    "Non-parametric methods\n",
    "- no explicit assumption but require large number of observations\n",
    "\n",
    "*a tradeoff: prediction accuracy and model interpretability*\n",
    "\n",
    "## Supervised and Unsepervised Learning\n",
    "Supervised learning: For each observation of the predictor measurement, there is an associated response measurement $y_i$.\n",
    "Unsupervised learning: For each observation, there's a vector of $x_i$, but no $y_i$.\n",
    "Semi-supervised learning problem: some of observations don't have $y_i$.\n",
    "\n",
    "## Assess Model Accuracy\n",
    "### Quality of Fit\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i-\\hat{f}(x_i))^2\n",
    "$$\n",
    "\n",
    "- We care about test MSE more than training MSE.\n",
    "- **Overfitting** happends if a method yields a small training MSE and large test MSE.\n",
    "- **Cross-validation**: a method for estimating the test MSE using the training data.\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "Decompostion of expected test MSE :\n",
    "$$\n",
    "E(y_0-\\hat{f}(x_0))^2 = var(\\hat{f}(x_0)) + [bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\n",
    "$$\n",
    "\n",
    "- **Variance** refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set, while **bias** refers to the error that is introduced by approximating a real-life problem.\n",
    "\n",
    "### Classification Setting\n",
    "**Training error rate** $\\frac{1}{n}\\sum_{i=1}^{n}I(y_i\\neq \\hat{y_i})$ is usually used to assess accuracy. We care about the smallest **test error**.\n",
    "\n",
    "The **Bayes Classifier** assigns a test observation with predictor vector $x_0$ to the class $j$ for which $Pr(Y=j|X=x_0)$ is largest. It minimized test error on average.\n",
    "\n",
    "In reality, we cannot get the conditional distribution, so we estimate it.(ex. **K-nearest neighbors classifer**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3f77e-7390-4063-ab8d-a2faaee22358",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Derivation of the Bias-Variance Trade-off\n",
    "\n",
    "Consider a regression problem where the data is generated as:\n",
    "\n",
    "$$\n",
    "y = f(x) + \\varepsilon, \\quad \\text{with } \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Our goal is to use a model $\\hat{f}(x)$ to estimate $y$, and minimize the prediction error.\n",
    "\n",
    "### 1. Define the Expected Mean Squared Error (MSE)\n",
    "\n",
    "$$\n",
    "\\text{MSE}(x) = \\mathbb{E}_{\\mathcal{D}, \\varepsilon} \\left[ \\left( y - \\hat{f}(x) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Substitute $ y = f(x) + \\varepsilon $:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(x) = \\mathbb{E}_{\\mathcal{D}, \\varepsilon} \\left[ \\left( f(x) + \\varepsilon - \\hat{f}(x) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Expand the square:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E} \\left[ \\left( f(x) - \\hat{f}(x) \\right)^2 + 2\\left( f(x) - \\hat{f}(x) \\right)\\varepsilon + \\varepsilon^2 \\right]\n",
    "$$\n",
    "\n",
    "Since \\( \\varepsilon \\) is independent of \\( \\hat{f}(x) \\) and \\( \\mathbb{E}[\\varepsilon] = 0 \\), the cross term vanishes:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E} \\left[ \\left( f(x) - \\hat{f}(x) \\right)^2 \\right] + \\mathbb{E}[\\varepsilon^2] = \\mathbb{E} \\left[ \\left( f(x) - \\hat{f}(x) \\right)^2 \\right] + \\sigma^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Decompose the Squared Error Term\n",
    "\n",
    "Now decompose the term $ \\mathbb{E} \\left[ \\left( \\hat{f}(x) - f(x) \\right)^2 \\right] $ by adding and subtracting the expected prediction $ \\mathbb{E}[\\hat{f}(x)] $:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( \\hat{f}(x) - f(x) \\right)^2 \\right] = \\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] + \\mathbb{E}[\\hat{f}(x)] - f(x) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Expand this expression:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] \\right)^2 \\right]\n",
    "+ \\left( \\mathbb{E}[\\hat{f}(x)] - f(x) \\right)^2\n",
    "+ 2 \\cdot \\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] \\right) \\left( \\mathbb{E}[\\hat{f}(x)] - f(x) \\right) \\right]\n",
    "$$\n",
    "\n",
    "The third term is zero because the inner expectation is 0:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]] = 0\n",
    "$$\n",
    "\n",
    "So we obtain:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( \\hat{f}(x) - f(x) \\right)^2 \\right]\n",
    "= \\underbrace{ \\left( \\mathbb{E}[\\hat{f}(x)] - f(x) \\right)^2 }_{\\text{Bias}^2}\n",
    "+ \\underbrace{ \\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] \\right)^2 \\right] }_{\\text{Variance}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Bias-Variance Decomposition Formula\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( y - \\hat{f}(x) \\right)^2 \\right]\n",
    "= \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "= (\\mathbb{E}[\\hat{f}(x)] - f(x))^2 + \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2] + \\sigma^2\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
