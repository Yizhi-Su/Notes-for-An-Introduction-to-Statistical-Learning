{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63287052-0fa9-4614-8606-d32ddb106f59",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Chapter 4: Classification\n",
    "\n",
    "## Logistic Regression (for binary outcomes)\n",
    "\n",
    "$$\n",
    "p(X)=Pr(Y=1|X)\n",
    "$$\n",
    "logistic function:\n",
    "$$\n",
    "p(X)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n",
    "$$\n",
    "\n",
    "*confounding problem*\n",
    "\n",
    "## Multinomial Logistic Regression\n",
    "\n",
    "$$\n",
    "\\Pr(Y = k \\mid X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1} x_1 + \\cdots + \\beta_{kp} x_p}}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1} x_1 + \\cdots + \\beta_{lp} x_p}}, \\quad \\text{for } k = 1, \\dots, K - 1\n",
    "$$\n",
    "\n",
    "And specifically for the Kth class (baseline)：\n",
    "\n",
    "$$\n",
    "\\Pr(Y = K \\mid X = x) = \\frac{1}{1 + \\sum_{l=1}^{K-1} e^{\\beta_{l0} + \\beta_{l1} x_1 + \\cdots + \\beta_{lp} x_p}}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "log(\\frac{Pr(Y=k|X=x}{Pr(Y=K|X=x}) = \\beta_{l0} + \\beta_{l1} x_1 + \\cdots + \\beta_{lp} x_p\n",
    "$$\n",
    "\n",
    "It shows that the **log odds** between any pair of classes is linear in the features.\n",
    "\n",
    "**Softmax coding** treats every variable symmetrically: $Pr(Y=k|X=x)=\\frac{\\beta_{l0} + \\beta_{l1} x_1 + \\cdots + \\beta_{lp} x_p}{\\sum_{l=1}^{K} e^{\\beta_{l0} + \\beta_{l1} x_1 + \\cdots + \\beta_{lp} x_p}}$, and $log(\\frac{Pr(Y=k|X=x}{Pr(Y=k'|X=x}) = (\\beta_{k0}-\\beta_{k'0}) + (\\beta_{k1}-\\beta_{k'1}) x_1 + \\cdots + (\\beta_{kp}-\\beta_{k'p} )x_p$\n",
    "\n",
    "## Generative Models for Classification\n",
    "Suppose that we wish to classify an observation into one of K classes.\n",
    "Denote $\\pi_k$ as the prior probability that a randomly chosen observation comes from the kth class, and let $f_k(X) = Pr(X|Y=k)$ be the density function of X for an observation that comes from the kth class. \n",
    "\n",
    "**Bayes’ theorem**:\n",
    "$$\n",
    "Pr(Y=k|X=x)=\\frac{\\pi_kf_k(x)}{\\sum_{l=1}^{K}\\pi_lf_l(x)}\n",
    "$$\n",
    "\n",
    "$\\pi_k$ can be easily estimated (see the fraction) if the sample is random from population. $f_k(x)$ can be estimated in muptiple ways.\n",
    "\n",
    "### 1. Linear Discriminant Analysis for p=1\n",
    "***Assumption 1*** $f_k(x)$ is **normal or Gaussian**, $f_k(x) = \\frac{1}{\\sqrt{2\\pi} \\, \\sigma_k} \\exp\\left( -\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2 \\right)$.\n",
    "\n",
    "***Assumption 2*** a **shared variance term** $\\sigma^2$ across classes.\n",
    "\n",
    "Then by *Bayes classfier*, assign the observation to the class for which $\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)$ is largest.\n",
    "**Linear Discriminant Analysis** uses the following estimates:\n",
    "$$\n",
    "\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i \n",
    "$$\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n_k} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n",
    "$$\n",
    "$$\n",
    "\\hat{pi}_k=n_k/n\n",
    "$$\n",
    "\n",
    "Then the estimate is $\\hat{\\delta}_k(x) = x \\cdot \\hat{\\mu}_k - \\frac{1}{2} \\hat{\\mu}_k^2 + \\log(\\hat{\\pi}_k)$, which shows the discriminant function $\\hat{\\delta}_k(x)$ are linear functions of $x$.\n",
    "\n",
    "### 2. Linear Discriminant Analysis for p>1 (multiple predictors)\n",
    "***Assumption*** $X = (X1,X2,...,Xp)$ is drawn from a **multi-variate Gaussian(or multivariate normal)distribution**,with a class-specific mean vector and a common covariance matrix. $f(x) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (x - \\boldsymbol{\\mu}) \\right)\n",
    "$\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = x^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_k + \\log \\pi_k\n",
    "$$\n",
    "\n",
    "**sensitivity** (predicted Yes/truly Yes) and **specificity** (predicted No/truly No)\n",
    "\n",
    "*ROC Curve*\n",
    "\n",
    "P.S. the tradeoff between two types of errors\n",
    "\n",
    "\n",
    "### 3. Quadratic Discriminant Analysis\n",
    "***Assumption***  Each class has its own covariance matrix. $X$ ~ $N (\\mu_k, \\boldsymbol{\\Sigma})$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_k(x) \n",
    "&= -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k \\\\\n",
    "&= -\\frac{1}{2} x^T \\Sigma_k^{-1} x + x^T \\Sigma_k^{-1} \\mu_k \n",
    "   - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k \n",
    "   - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "LDA or QDA? - think about *bias-vatiance tradeoff*\n",
    "\n",
    "### 4. Naive Bayes\n",
    "***Assumption*** Within the kth class, the p predictors are independent. $f_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdot \\cdot \\cdot \\times f_{kp}(x_p)$, where $f_{kj}$ is the density function of the jth predictor among observations in the kth class.\n",
    "\n",
    "$$\n",
    "Pr (Y=k|X=x) = \\frac{\\pi_k\\times f_{k1}(x_1)\\times f_{k2}(x_2)\\times \\cdot \\cdot \\cdot \\times f_{kp}(x_p)}{\\sum_{l=1}^K \\pi_l\\times f_{l1}(x_1)\\times f_{l2}(x_2)\\times \\cdot \\cdot \\cdot \\times f_{lp}(x_p)}\n",
    "$$\n",
    "\n",
    "How to estimate the one-dimensional density function $f_{kj}$? Several options:\n",
    "\n",
    "- If $X_j$ is quantitative, assume that $X_j|Y = k$ ~ $N(\\mu_{jk}, \\sigma_{jk}^2)$. With in each class, the jth predictor is drawn from a (univariate) normal distribution.\n",
    "- If $X_j$ is quantitative, use non-parametric methods to estimate $f_{kj}$.\n",
    "- If $X_j$ is qualitative, then we can simply count the proportion of training observations for the jth predictor corresponding to each class.\n",
    "\n",
    "## Generalizaed linear model\n",
    "$$\n",
    "\\eta(E(Y|X_1,..., X_p)) = \\beta_0 + \\beta_1X_1 + \\cdot \\cdot \\cdot + \\beta_pX_p\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a5e77-2c21-4f63-a7b5-e18cb6077ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
