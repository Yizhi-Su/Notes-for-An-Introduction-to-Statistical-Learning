{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe4d399-36f6-4803-b669-579d7ee4db11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Chapter 6: Linear Model Selection and Regularization\n",
    "extend the linear framework; alternative preocedures (instead of least squares) that can yield better **prediction acuracy** and **model interpretability**.\n",
    "\n",
    "three classes of methods: \n",
    "- **subset selection**: identify a subset of predictors first\n",
    "- **shrinkage (regularization)**: the estimated coeﬀicients are shrunken towards zero relative to the least squares estimates\n",
    "- **dimenstion reduction**: project the p predictors into an M-dimensional subspace, where $M < p$. (This is achieved by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares.)\n",
    "\n",
    "Curse of Dimensinality:\n",
    "1) Regularization or shrinkage plays a key role in high-dimensional problems.\n",
    "2) Appropriate tuning parameter selection is crucial for good predictive performance\n",
    "3) The test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.\n",
    "\n",
    "## Subset Selection\n",
    "\n",
    "### Best Subset Selection\n",
    "Idea: first, fit a separate least squares regression for each possible combination of the p predictors; second, identify the best one.\n",
    "\n",
    "1. Let $M_0$ denote the *null model*, which contains no predictors. This model simply predicts the sample mean for each observation.\n",
    "\n",
    "2. For $k = 1, 2, \\ldots, p$:\n",
    "\n",
    "   (a) Fit all $\\binom{p}{k}$ models that contain exactly $k$ predictors.\n",
    "\n",
    "   (b) Pick the best among these $\\binom{p}{k}$ models, and call it $M_k$. Here *best* is defined as having the smallest RSS, or equivalently largest $R^2$.\n",
    "\n",
    "3. Select a single best model from among $M_0, \\ldots, M_p$ using the prediction error on a validation set, $C_p$ (AIC), BIC, or adjusted $R^2$. Or use the cross-validation method.\n",
    "\n",
    "Disadvantages: (1) computational limitation (2) only for least squares linear regression\n",
    "\n",
    "### Stepwise Selection\n",
    "\n",
    "#### Forward Stepwise Selection\n",
    "\n",
    "1. Let $M_0$ denote the *null model*, which contains no predictors.\n",
    "\n",
    "2. For $k = 0, \\ldots, p - 1$:\n",
    "\n",
    "   (a) Consider all $p - k$ models that augment the predictors in $M_k$ with one additional predictor.\n",
    "\n",
    "   (b) Choose the *best* among these $p - k$ models, and call it $M_{k+1}$. Here *best* is defined as having smallest RSS or highest $R^2$.\n",
    "\n",
    "3. Select a single best model from among $M_0, \\ldots, M_p$ using the prediction error on a validation set, $C_p$ (AIC), BIC, or adjusted $R^2$. Or use the cross-validation method.\n",
    "\n",
    "#### Backward Stepwise Selection\n",
    "\n",
    "1. Let $M_p$ denote the *full model*, which contains all $p$ predictors.\n",
    "\n",
    "2. For $k = p, p - 1, \\ldots, 1$:\n",
    "\n",
    "   (a) Consider all $k$ models that contain all but one of the predictors in $M_k$, for a total of $k - 1$ predictors.\n",
    "\n",
    "   (b) Choose the *best* among these $k$ models, and call it $M_{k-1}$. Here *best* is defined as having smallest RSS or highest $R^2$.\n",
    "\n",
    "3. Select a single best model from among $M_0, \\ldots, M_p$ using the prediction error on a validation set, $C_p$ (AIC), BIC, or adjusted $R^2$. Or use the cross-validation method.\n",
    "\n",
    "p.s. Backward stepwise is viable when $p$ is very large.\n",
    "\n",
    "#### Hybrid Approaches\n",
    "\n",
    "Idea: Variables are added to the model sequentially, in analogy to forward selection. After adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit.\n",
    "\n",
    "### Choosing the Optimal Model\n",
    "A fact: RSS and $R^2$ are not suitable for selecting the best model among a collection of models with different numbers of predictors.\n",
    "\n",
    "Two approaches to estimate the test error:\n",
    "\n",
    "1) indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting\n",
    "\n",
    "| Criterion        | Formula (OLS case)                                                                                     | Parameters                                                                                         | Intuition                                                                                          |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|\n",
    "| $C_p$            | $C_p = \\frac{\\mathrm{RSS}}{\\hat{\\sigma}^2} + 2d$                                                       | $\\mathrm{RSS}$ = residual sum of squares; $\\hat{\\sigma}^2$ = estimate of error variance; $d$ = number of parameters (incl. intercept) | Balances fit and complexity; aims to select models with low prediction error.                     |\n",
    "| AIC              | $\\mathrm{AIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2d$                                        | $n$ = sample size; $d$ = number of parameters; $\\mathrm{RSS}$ = residual sum of squares            | Information-theoretic measure; penalizes complexity with $2d$, favors models that explain the data well. |\n",
    "| BIC              | $\\mathrm{BIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + d\\log(n)$                                  | $n$ = sample size; $d$ = number of parameters; $\\mathrm{RSS}$ = residual sum of squares            | Stronger penalty than AIC when $n > 7$; more conservative, tends to choose simpler models.         |\n",
    "| Adjusted $R^2$   | $\\bar{R}^2 = 1 - \\frac{\\mathrm{RSS}/(n - d - 1)}{\\mathrm{TSS}/(n - 1)}$                                 | $\\mathrm{TSS}$ = total sum of squares; $n$ = sample size; $d$ = number of predictors (excl. intercept) | Measures proportion of variance explained, adjusted for number of predictors; discourages overfitting. |\n",
    "\n",
    "  \n",
    "2) directly estimate the test error, using either a validation set approach or a cross-validation approach\n",
    "\n",
    "  **one-standard-error rule**: We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.\n",
    "\n",
    "## Shrinkage Methods\n",
    "\n",
    "### Ridge\n",
    "\n",
    "minimize:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0, \\beta} \\ \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\ + \\ \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "p.s. We can use cross-validation to choose the proper $\\lambda$ (tuning parameter).\n",
    "\n",
    "Note the shrinkage penalty is applied to $β_1,...,β_p$, but not to the intercept $β_0$.\n",
    "\n",
    "It can create a challenge in model interpretation in settings in which the number of variables p is quite large. (then consider LASSO)\n",
    "\n",
    "### LASSO\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0, \\beta} \\ \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\ + \\ \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right|\n",
    "$$\n",
    "\n",
    "### Alternative Formulation\n",
    "\n",
    "We can show that the lasso and ridge regression coefficient estimates solve the problems\n",
    "\n",
    "**Lasso:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{\\beta_0, \\beta} \\ \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\\\\n",
    "&\\text{subject to} \\quad \\sum_{j=1}^p |\\beta_j| \\le s\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Ridge:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min_{\\beta_0, \\beta} \\ \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\\\\n",
    "&\\text{subject to} \\quad \\sum_{j=1}^p \\beta_j^2 \\le s\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "vs **Best Subset Selection:** (coputiational infeasible)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{\\beta_0, \\beta} \\quad \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 \\\\\n",
    "& \\text{subject to} \\quad \\sum_{j=1}^p \\mathbf{1}_{\\{\\beta_j \\neq 0\\}} \\le k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "*Ridge vs LASSO*: In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coeﬀicients, and the remaining predictors have coeﬀicients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coeﬀicients of roughly equal size. \n",
    "\n",
    "### Select the tuning parameter\n",
    "\n",
    "choose a grid of $λ$ values, and compute the cross-validation error for each value of $λ$;\n",
    "then select the tuning parameter value for which the cross-validation error is smallest;\n",
    "refit the model using all of the available observations and the selected value of the tuning parameter.\n",
    "\n",
    "## Dimesnsion Reduction Methods\n",
    "Idea: transform the predictors and then fit a least squares model using the transformed variables\n",
    "\n",
    "### Principal Components Regression (PCA)\n",
    "PCA seeks a projection direction that maximizes the variance of the data projected onto it. A larger variance means that the direction contains richer information and can better distinguish between samples.\n",
    "\n",
    "### The Principal Components Regression Approach (PCR)\n",
    "construct the first M principal components, $Z_1,...,Z_M$; use these components as the predictors in a linear regression model that is fit using least squares\n",
    "\n",
    "PCR does not result in the development of a model that relies upon a small set of the original features. (In this sense, PCR is more closely related to ridge regression than to the lasso.)\n",
    "\n",
    "Directions are identified in an **unsupervised** way, since the response Y is not used to help determine the principal component directions. So, there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.\n",
    "\n",
    "\n",
    "### Partial Least Squares (PLS)\n",
    "$-$ Standardize predictors. \n",
    "\n",
    "$-$ Compute first PLS direction $Z_1$ by weighting predictors based on their regression coefficients with $Y$ (PLS identifies directions in a **supervised** way that makes use of $Y$).\n",
    "\n",
    "$-$ Regress predictors on $Z_1$, take residuals to remove explained variation.\n",
    "\n",
    "$-$ Compute next PLS direction $Z_2$ using residuals.\n",
    "\n",
    "$-$ Repeat to get orthogonal components $Z_1, Z_2, ..., Z_M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de962b-93a3-450d-a9e6-15278afefda2",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Ridge and LASSO\n",
    "\n",
    "From a geometric perspective, because the constraint region of Lasso has sharp corners (located on the coordinate axes), the optimization solution is more likely to fall exactly on some axes, resulting in sparse solutions (some coefficients being exactly zero); whereas the Ridge constraint is a smooth circle, which rarely yields coefficients that are exactly zero.\n",
    "\n",
    "## Compare Dimension Reduction Methods\n",
    "\n",
    "| Method | Objective                              | Basis for Dimension Reduction       |\n",
    "|--------|--------------------------------------|------------------------------------|\n",
    "| PCA    | Maximize variance of predictors      | Covariance matrix of predictors X  |\n",
    "| PCR    | Principal component regression based on PCA | Principal components of predictors X |\n",
    "| PLS    | Maximize covariance between predictors and response | Covariance between predictors X and response Y |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
