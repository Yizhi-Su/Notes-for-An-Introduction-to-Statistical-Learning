{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eff393c-4f06-4e47-a757-926972927738",
   "metadata": {},
   "source": [
    "# Chapter 5: Resampling Methods\n",
    "## Cross-Validation\n",
    "### Validation Set Approach\n",
    "Idea: randomly splitting the available set into a *training set* and a *validation set*; the model is fit on the training set and used to predict in the validation set; validation set error rate (typically MSE) provides an estimate of the test error rate\n",
    "\n",
    "Two drawbacks:\n",
    "1) The validation estimate of the test error rate can be highly variable across different training and validation set.\n",
    "2) Only data in the training set is used to fit the model, hence the validation wet error rate tend to overestimate the test error rate for the model fit on the netire data set.\n",
    "\n",
    "### Leave-One-Out Cross Validation\n",
    "Idea: every time, a single observation is used in the validation set; repeat the approach n times.\n",
    "Then, we have the **LOOCV** estimate for the test MSE:\n",
    "$$\n",
    "CV_n=\\frac{1}{n}\\sum_{i=1}^n MSE_i\n",
    "$$\n",
    "\n",
    "Two Advantages:\n",
    "1) tends not to overestimate the test error\n",
    "2) no randomness of results\n",
    "\n",
    "ps. the **leverage statistic** in (3.37): $h_i=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum_{i'=1}^n(x_{i'}-\\bar{x})^2}$.\n",
    "he leverage statistic $h_i$ is always between $1/n$ and $1$, and the average leverage for all the observations is always equal to $(p + 1)/n$. \n",
    "\n",
    "**With least squares linear or polynomial regression**, a shortcut for LOOCV MSE is:\n",
    "$$\n",
    "CV_n=\\frac{1}{n}\\sum_{i=1}^{n}(\\frac{y_i-\\hat{y_i}}{1-h_i})^2\n",
    "$$\n",
    "where $\\hat{y_i}$ is the ith fitted value from the original least squares fit. \n",
    "\n",
    "* This formula allows us to fit the model only once (using all the data) and still estimate the prediction error for each leave-one-out observation.\n",
    "* The shortcut does not hold in general.\n",
    "\n",
    "\n",
    "\n",
    "### k-Fold Cross-Validation\n",
    "Idea: randomly divide the set of observations into k groups of approximately equal size; the first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds; then we have $MSE_1$ computed on the observations in the held-out fold; repeat for k times.\n",
    "$$\n",
    "CV_k=\\frac{1}{k}\\sum_{i=1}^k MSE_i\n",
    "$$\n",
    "\n",
    "#### Bias-Variance Trade-Off for k-Fold Cross-Validation\n",
    "\n",
    "Bias: k-fold CV contains less data in the training set → more biased\n",
    "\n",
    "Varianve: LOOCV is trained on an almost identical set of observations → outputs are highly (positively) correlated with each other → the test error estimate resulting from LOOCV tends to have higher variance (see details in Notes)\n",
    "\n",
    "### Cross-Validation on Classification Problems\n",
    "Idea: use **the number of misclassified observations** to quantify the test error\n",
    "LOOCV error rate: $CV_n=\\frac{1}{n}\\sum_{i=1}^n ERR_i$, where $Err_i = I(y_i\\ne \\hat{y_i})$.\n",
    "\n",
    "## Bootstrap\n",
    "Idea: quantify the uncertainty associated with a given estimator or statistical learning method\n",
    "\n",
    "Steps:\n",
    "1) a data set set $Z$ with n observations; randomly select n observations **with replacement** from the data set produce a bootstrap data set, call it $Z^{*1}$\n",
    "2) produce a bootstrap estimate for $\\alpha$, call it $\\hat{\\alpha}^{*1}$\n",
    "3) repeat the procedure for $B$ (large number) times\n",
    "\n",
    "Then compute the **standard error of these bootstrap estimates**:\n",
    "$$\n",
    "\\mathrm{SE}_B(\\hat{\\alpha}) = \\sqrt{ \\frac{1}{B - 1} \\sum_{r = 1}^B \\left( \\hat{\\alpha}^{*r} - \\frac{1}{B} \\sum_{r' = 1}^B \\hat{\\alpha}^{*r'} \\right)^2 }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900e410-481a-4483-8eb4-9d8ab6c039a2",
   "metadata": {},
   "source": [
    "# Notes\n",
    "## Derivation of Leave-One-Out Cross-Validation (LOOCV) MSE\n",
    "\n",
    "Consider the linear modesl:\n",
    "$$\n",
    "\\mathbf{y} = X \\beta + \\varepsilon, \\quad \\text{with } \\hat{\\beta} = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "with\n",
    "$$\n",
    "\\hat{y} = X \\hat{\\beta}, \\quad e = y - \\hat{y}\n",
    "$$\n",
    "\n",
    "The predicted value for the $i$th observation:\n",
    "$$\n",
    "\\hat{y}_i = x_i^T \\hat{\\beta}\n",
    "$$\n",
    "\n",
    "The leverage statistic $h_i$, i.e. diagonal elements of $H = X (X^T X)^{-1} X^T$:\n",
    "$$\n",
    "h_i = x_i^T (X^T X)^{-1} x_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Leave-one-out cross-validation:\n",
    "$$\n",
    "CV_n = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i^{(-i)} \\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "By Sherman-Morrison equation, we have:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(-i)} = \\hat{y}_i - \\frac{e_i}{1 - h_i}\n",
    "$$\n",
    "\n",
    "Then the residual is\n",
    "\n",
    "$$\n",
    "y_i - \\hat{y}_i^{(-i)} = \\frac{e_i}{1 - h_i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Put it into the defeinition of LOOCV MSE, then we have\n",
    "\n",
    "$$\n",
    "CV_n = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\n",
    "= \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{e_i}{1 - h_i} \\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Why Highly Correlated Variables Increase the Variance of the Mean ？\n",
    "\n",
    "Consider $ X_1, X_2, \\dots, X_n $, and define their mean:\n",
    "\n",
    "$$\n",
    "\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "$$\n",
    "\n",
    "The variance of $ \\bar{X} $ is:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\text{Cov}(X_i, X_j)\n",
    "$$\n",
    "\n",
    "This shows that the variance of the average depends on **both** the individual variances and the covariances between variables.\n",
    "\n",
    "---\n",
    "\n",
    "### Case 1: Independent Variables\n",
    "\n",
    "If $ X_i $ are i.i.d. with variance $ \\sigma^2 $, then:\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X_i, X_j) = 0 \\quad \\text{for } i \\ne j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\bar{X}) = \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "→ Averaging reduces variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: Perfectly Correlated Variables\n",
    "\n",
    "If $ X_1 = X_2 = \\cdots = X_n $, then:\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X_i, X_j) = \\sigma^2 \\quad \\text{for all } i, j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\bar{X}) = \\frac{1}{n^2} \\cdot n^2 \\cdot \\sigma^2 = \\sigma^2\n",
    "$$\n",
    "\n",
    "→ Averaging **does not** reduce variance at all.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> The more correlated the variables, the less averaging helps reduce variance.\n",
    "\n",
    "This is why the average of many **highly correlated** quantities can have **higher variance** than the average of many **uncorrelated** ones.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
