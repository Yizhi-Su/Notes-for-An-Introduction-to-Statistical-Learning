{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cafc53-e96a-4580-8eff-cf5e9c0008fc",
   "metadata": {},
   "source": [
    "# Chapter 8: Tree-Based Methods\n",
    "## The Basics of Decision Trees\n",
    "### Regression Trees\n",
    "The general process(idea) of building a regression tree:\n",
    "1. We divide the predictor space — that is, the set of possible values for $X_1, X_2, . . . , X_p$ — into $J$ distinct and non-overlapping regions, $R_1,R_2,...,R_J$.\n",
    "2. For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.\n",
    "\n",
    "How to construct regions?\n",
    "The goal is to find boxes $R_1, . . . , R_J$ that minimize the RSS, given by $\\sum_{j=1}^{J} \\sum_{i \\in R_j} \\left( y_i - \\hat{y}_{R_j} \\right)^2$ → computationally infeasible\n",
    "\n",
    "**Recursive Binary Splitting** - a top-down *greedy* approach\n",
    "- select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions ${X|X_j < s}$ and ${X|X_j ≥ s}$ leads to the greatest possible reduction in RSS\n",
    "- In greater detail, for any $j$ and $s$, we define the pair of half-planes $R_1(j, s) = {X|X_j < s}$ and $R_2(j, s) = {X|X_j ≥ s}$, and we seek the value of $j$ and $s$ that minimize the equation\n",
    "$$\n",
    "\\sum_{i : x_i \\in R_1(j,s)} \\left( y_i - \\hat{y}_{R_1} \\right)^2\n",
    "+ \\sum_{i : x_i \\in R_2(j,s)} \\left( y_i - \\hat{y}_{R_2} \\right)^2\n",
    "$$\n",
    "- repeat the process, and split one of the two previously identified regions\n",
    "- contibue the process until a stopping criterion is reached\n",
    "\n",
    "**Prune Trees**\n",
    "\n",
    "The process above is likely to overfit the data.\n",
    "\n",
    "→ One alternative: to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold;\n",
    "But, a seemingly worthless split early on in the tree might be followed by a very good split—that is, a split that leads to a large reduction in RSS later on.\n",
    "\n",
    "A better strategy: to grow a very large tree $T_0$, and then *prune* it back in order to obtain a *subtree*.\n",
    "\n",
    "How to decide the way to prune the tree? - Intuitively, to select a subtree that leads to the lowest test error rate.\n",
    "\n",
    "cross-validation? - cumbersome\n",
    "\n",
    "***Cost complexity pruning (weakest link pruning)*** - consider a sequence of trees indexed by a nonnegative tuning parameter $α$. For each value of $α$ there corresponds a subtree $T ⊂ T_0$ such that $\\sum_{m=1}^{|T|} \\sum_{i:x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|$ is as small as possible. $|T|$ is the number of terminal nodes of the tree $T$. The tuning parameter $α$ controls a trade-off between the subtree’s com- plexity and its fit to the training data.\n",
    "\n",
    "---\n",
    "**Algorithm: Build a Regression Tree**\n",
    "\n",
    "1. Use *recursive binary splitting* to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n",
    "2. Apply *cost complexity pruning* to the large tree in order to obtain a sequence of best subtrees, as a function of $α$.\n",
    "3. Use K-fold cross-validation to choose $α$. That is, divide the training observations into K folds. For each $k = 1, . . . , K$:\n",
    "(a) Repeat Steps 1 and 2 on all but the kth fold of the training data.\n",
    "(b) Evaluate the mean squared prediction error on the data in the left-out $k$th fold, as a function of $α$.\n",
    "Average the results for each value of $α$, and pick $α$ to minimize the average error.\n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $α$.\n",
    "---\n",
    "\n",
    "### Classification Trees\n",
    "In the classification setting, RSS cannot be used as a criterion for making the binary splits.\n",
    "\n",
    "Alternatives:\n",
    "\n",
    "- **Classification Error Rate**: $E=1-\\max\\limits_{k}(\\hat{p_{mk}})$; the fraction of the training observations in that region that do not belong to the most common class; However, it turns out that classification error is not suﬀiciently sensitive for tree-growing.\n",
    "- **Gini Index**: $G = \\sum_k^K \\hat{p_{mk}}(1-\\hat{p_{mk}})$; a measure of node purity — a small value indicates that a node contains predominantly observations from a single class.\n",
    "- **entropy**: $D=- \\sum_k^K \\hat{p_{mk}}log\\hat{p_{mk}}$; similar to Gini Index numerically.\n",
    "\n",
    "Any of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal.\n",
    "\n",
    "## Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees\n",
    "By aggregating many decision trees, using ensmble methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved.\n",
    "\n",
    "An *ensemble method* is an approach that combines many simple “building block” models in order to obtain a single and potentially powerful model. \n",
    "\n",
    "### Bagging\n",
    "Idea: Averaging a set of observations reduces variance. - calculate $\\hat{f}^1(x),\\hat{f}^2(x),...,\\hat{f}^B(x)$ using B separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by $\\hat{f}_{avg}(x)=\\frac{1}{B}\\sum_{b=1}^B\\hat{f}^b(x)$.\n",
    "\n",
    "In pracctice, we can bootstrap by taking repeated samples from the (single) training data set. Then $\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)$\n",
    "\n",
    "When applying bagging to decision trees, each of the B bootstrapped trees are not pruned, so each individual tree has high variance, but low bias.\n",
    "\n",
    "In qualitative situations, we can record the class predicted by each of the B trees, and take a *majority vote*: the overall prediction is the most commonly occurring majority class among the B predictions.\n",
    "\n",
    "*Variable Importance*: Although the collection of bagged trees is much more diﬀicult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). \n",
    "\n",
    "### Random Forests\n",
    "As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. A fresh sample of $m$ predictors is taken at each split, and typically we choose $m ≈ \\sqrt{p}$.\n",
    "\n",
    "'Suppose that there is one very strong predictor in the data set, along with a num- ber of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated. Un- fortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quan- tities.' - Random forests overcome this problem by forcing each split to consider only a subset of the predictors.\n",
    "\n",
    "### Boosting\n",
    "Idea: Trees are grown sequentially - each tree is grown using information from previously grown trees. \n",
    "\n",
    "---\n",
    "**Algorithm: Boosting for Regression Trees**\n",
    "1. Set $\\hat{f(x)} = 0$ and $r_i = y_i$ for all $i$ in the training set.  \n",
    "\n",
    "2. For $b = 1,2,\\ldots,B$, repeat:  \n",
    "   a. Fit a tree $\\hat{f}^{\\,b}$ with $d$ splits ($d+1$ terminal nodes) to the training data $(X, r)$.  \n",
    "   b. Update $\\hat{f}$ by adding in a shrunken version of the new tree:  \n",
    "      $\n",
    "      \\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^{\\,b}(x)\n",
    "      $  \n",
    "   c. Update the residuals:  \n",
    "      $\n",
    "      r_i \\leftarrow r_i - \\lambda \\hat{f}^{\\,b}(x).\n",
    "      $  \n",
    "\n",
    "3. Output the boosted model:  \n",
    "   $\n",
    "   \\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^{\\,b}(x)\n",
    "   $\n",
    "---\n",
    "\n",
    "### Bayesian Additive Regression Trees\n",
    "Idea: Each tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.\n",
    "\n",
    "---\n",
    "**Algorithm:  Bayesian Additive Regression Trees (BART)**\n",
    "\n",
    "1. Initialize  \n",
    "\n",
    "$$\n",
    "\\hat f^{\\,1}_1(x) = \\hat f^{\\,1}_2(x) = \\cdots = \\hat f^{\\,1}_K(x) = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$\n",
    "\n",
    "2. Compute the initial fit  \n",
    "\n",
    "$$\n",
    "\\hat f^{\\,1}(x) = \\sum_{k=1}^K \\hat f^{\\,1}_k(x) = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "$$\n",
    "\n",
    "3. For $b = 2, \\ldots, B$:  \n",
    "\n",
    "   (a) For $k = 1,2,\\ldots,K$:  \n",
    "\n",
    "   i. For $i = 1,\\ldots,n$, compute the current partial residual  \n",
    "\n",
    "   $$\n",
    "   r_i = y_i - \\sum_{k' < k} \\hat f^{\\,b}_{k'}(x_i) - \\sum_{k' > k} \\hat f^{\\,b-1}_{k'}(x_i)\n",
    "   $$  \n",
    "\n",
    "   ii. Fit a new tree $\\hat f^{\\,b}_k(x)$ to $r_i$, by randomly perturbing the $k$-th tree from the previous iteration, $\\hat f^{\\,b-1}_k(x)$.  \n",
    "   Perturbations that improve the fit are favored.  \n",
    "\n",
    "   (b) Compute the updated ensemble  \n",
    "\n",
    "   $$\n",
    "   \\hat f^{\\,b}(x) = \\sum_{k=1}^K \\hat f^{\\,b}_k(x)\n",
    "   $$\n",
    "\n",
    "4. After $L$ burn-in samples, compute the posterior mean  \n",
    "\n",
    "$$\n",
    "\\hat f(x) = \\frac{1}{B-L} \\sum_{b=L+1}^B \\hat f^{\\,b}(x)\n",
    "$$\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
